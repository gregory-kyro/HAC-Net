{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bYFW5V5MhjOJ",
        "DN8fWkY00lle"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Train HAC-Net"
      ],
      "metadata": {
        "id": "u5edm2k350s1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook allows one to train HAC-Net with any dataset that we provide, or any other dataset of the same format and preprocessing requirements. In this notebook, we train with the training set that corresponds to testing on the PDBbind 2016 core set. "
      ],
      "metadata": {
        "id": "rjEdaZgf53XS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up Notebook"
      ],
      "metadata": {
        "id": "N0qKg-QThfE_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxIUYShHahCF"
      },
      "outputs": [],
      "source": [
        "# install torch packages necessary for GCNs\n",
        "!pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "import h5py\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from torch_geometric.nn.conv import MessagePassing, GatedGraphConv\n",
        "from torch_geometric.nn import global_add_pool\n",
        "from torch_geometric.utils import add_self_loops\n",
        "from torch_geometric.nn.aggr import AttentionalAggregation\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "from torch.nn.parallel import DataParallel, DistributedDataParallel\n",
        "from torch_geometric.nn import DataParallel as GeometricDataParallel\n",
        "from torch_geometric.data import DataListLoader, Data\n",
        "from torch_geometric.utils import dense_to_sparse\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import scipy as sp\n",
        "from scipy.stats import *\n",
        "from sklearn.metrics import *\n",
        "import random\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from torch._C import NoneType\n",
        "from torch.optim import Adam, RMSprop, lr_scheduler"
      ],
      "metadata": {
        "id": "kiP7T77dqOEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HbUBCC0Tbuoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Containers"
      ],
      "metadata": {
        "id": "bYFW5V5MhjOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' Define a class to contain the data that will be included in the dataloader \n",
        "sent to the GCN model '''\n",
        "\n",
        "class GCN_Dataset(Dataset):\n",
        "  \n",
        "    def __init__(self, data_file):\n",
        "        super(GCN_Dataset, self).__init__()\n",
        "        self.data_file = data_file\n",
        "        self.data_dict = {}\n",
        "        self.data_list = []\n",
        "        \n",
        "        # retrieve PDB id's and affinities from hdf file\n",
        "        with h5py.File(data_file, 'r') as f:\n",
        "            for pdbid in f.keys():\n",
        "                affinity = np.asarray(f[pdbid].attrs['affinity']).reshape(1, -1)\n",
        "                self.data_list.append((pdbid, affinity))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "\n",
        "        \n",
        "        if item in self.data_dict.keys():\n",
        "            return self.data_dict[item]\n",
        "\n",
        "        pdbid, affinity = self.data_list[item]\n",
        "        node_feats, coords = None, None\n",
        "\n",
        "        coords=h5py.File(self.data_file,'r')[pdbid][:,0:3]\n",
        "        dists=pairwise_distances(coords, metric='euclidean')\n",
        "        \n",
        "        self.data_dict[item] = (pdbid, dists)\n",
        "        return self.data_dict[item]"
      ],
      "metadata": {
        "id": "Sak4Wo4_0ZoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Define a class to contain the data that will be included in the dataloader \n",
        "sent to the 3D-CNN \"\"\"\n",
        "\n",
        "class CNN_Dataset(Dataset):\n",
        "\n",
        "\tdef __init__(self, hdf_path, feat_dim=22):\n",
        "\t\tsuper(CNN_Dataset, self).__init__()\n",
        "\t\tself.hdf_path = hdf_path\n",
        "\t\tself.feat_dim = feat_dim\n",
        "\t\tself.hdf = h5py.File(self.hdf_path, 'r')\n",
        "\t\tself.data_info_list = []\n",
        "    # append PDB id and affinity label to data_info_list\n",
        "\t\tfor pdbid in self.hdf.keys():\n",
        "\t\t\taffinity = float(self.hdf[pdbid].attrs['affinity'])\n",
        "\t\t\tself.data_info_list.append([pdbid, affinity])\n",
        "\n",
        "\tdef close(self):\n",
        "\t\tself.hdf.close()\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\tcount = len(self.data_info_list)\n",
        "\t\treturn count\n",
        "\t\t\n",
        "\tdef get_data_info_list(self):\n",
        "\t\treturn self.data_info_list\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\tpdbid, affinity = self.data_info_list[idx]\n",
        "\n",
        "\t\tdata = self.hdf[pdbid][:]\n",
        "\t\tx = torch.tensor(data)\n",
        "\t\tx = x.permute(3,0,1,2)\n",
        "\t\ty = torch.tensor(np.expand_dims(affinity, axis=0))\n",
        "\t\treturn x,y, pdbid"
      ],
      "metadata": {
        "id": "j-25riu-fHR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Define a class to contain the extracted 3D-CNN features that will be included in the dataloader \n",
        "sent to the fully-connected network \"\"\"\n",
        "\n",
        "class Linear_Dataset(Dataset):\n",
        "\tdef __init__(self, npy_path, feat_dim=22):\n",
        "\t\tsuper(Linear_Dataset, self).__init__()\n",
        "\t\tself.npy_path = npy_path\n",
        "\t\tself.input_feat_array = np.load(npy_path, allow_pickle=True)[:,:-1].astype(np.float32)\n",
        "\t\tself.input_affinity_array = np.load(npy_path, allow_pickle=True)[:,-1].astype(np.float32)\n",
        "\t\tself.data_info_list = []\n",
        "    \n",
        "\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\tcount = self.input_feat_array.shape[0]\n",
        "\t\treturn count\n",
        "\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\tdata, affinity = self.input_feat_array[idx], self.input_affinity_array[idx]\n",
        "\n",
        "\t\tx = torch.tensor(data)\n",
        "\t\ty = torch.tensor(np.expand_dims(affinity, axis=0))\n",
        "\t\treturn x,y"
      ],
      "metadata": {
        "id": "pCydT1gvqWfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture"
      ],
      "metadata": {
        "id": "DN8fWkY00lle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Define GCN architecture class \"\"\"\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, gather_width=128, prop_iter=4, dist_cutoff=3.5):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        #define distance cutoff\n",
        "        self.dist_cutoff=torch.Tensor([dist_cutoff])\n",
        "        if torch.cuda.is_available():\n",
        "            self.dist_cutoff = self.dist_cutoff.cuda()\n",
        "\n",
        "        #Attentional aggregation\n",
        "        self.gate_net = nn.Sequential(nn.Linear(in_channels, int(in_channels/2)), nn.Softsign(), nn.Linear(int(in_channels/2), int(in_channels/4)), nn.Softsign(), nn.Linear(int(in_channels/4),1))\n",
        "        self.attn_aggr = AttentionalAggregation(self.gate_net)\n",
        "        \n",
        "        #Gated Graph Neural Network\n",
        "        self.gate = GatedGraphConv(in_channels, prop_iter, aggregation=self.attn_aggr)\n",
        "\n",
        "        #Simple neural networks for use in asymmetric attentional aggregation\n",
        "        self.attn_net_i=nn.Sequential(nn.Linear(in_channels * 2, in_channels), nn.Softsign(),nn.Linear(in_channels, gather_width), nn.Softsign())\n",
        "        self.attn_net_j=nn.Sequential(nn.Linear(in_channels, gather_width), nn.Softsign())\n",
        "\n",
        "        #Final set of linear layers for making affinity prediction\n",
        "        self.output = nn.Sequential(nn.Linear(gather_width, int(gather_width / 1.5)), nn.ReLU(), nn.Linear(int(gather_width / 1.5), int(gather_width / 2)), nn.ReLU(), nn.Linear(int(gather_width / 2), 1))\n",
        "\n",
        "    def forward(self, data):\n",
        "\n",
        "        #Move data to GPU\n",
        "        if torch.cuda.is_available():\n",
        "            data.x = data.x.cuda()\n",
        "            data.edge_attr = data.edge_attr.cuda()\n",
        "            data.edge_index = data.edge_index.cuda()\n",
        "            data.batch = data.batch.cuda()\n",
        "\n",
        "        # allow nodes to propagate messages to themselves\n",
        "        data.edge_index, data.edge_attr = add_self_loops(data.edge_index, data.edge_attr.view(-1))\n",
        "\n",
        "        # restrict edges to the distance cutoff\n",
        "        row, col = data.edge_index\n",
        "        mask = data.edge_attr <= self.dist_cutoff\n",
        "        mask = mask.squeeze()\n",
        "        row, col, edge_feat = row[mask], col[mask], data.edge_attr[mask]\n",
        "        edge_index=torch.stack([row,col],dim=0)\n",
        "\n",
        "        # propagation\n",
        "        node_feat_0 = data.x\n",
        "        node_feat_1 = self.gate(node_feat_0, edge_index, edge_feat)\n",
        "        node_feat_attn = torch.nn.Softmax(dim=1)(self.attn_net_i(torch.cat([node_feat_1, node_feat_0], dim=1))) * self.attn_net_j(node_feat_0)\n",
        "\n",
        "        # globally sum features and apply linear layers\n",
        "        pool_x = global_add_pool(node_feat_attn, data.batch)\n",
        "        prediction = self.output(pool_x)\n",
        "\n",
        "        return prediction"
      ],
      "metadata": {
        "id": "xSPBWkAY0nr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Define 3D-CNN architecture class \"\"\"\n",
        "\n",
        "class Model_3DCNN(nn.Module):\n",
        "\n",
        "  def __conv_filter__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "    conv_filter = nn.Sequential(nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=True), nn.ReLU(inplace=True), nn.BatchNorm3d(out_channels))\n",
        "    return conv_filter\n",
        "\n",
        "  def __init__(self, feat_dim=19, output_dim=1, use_cuda=True):\n",
        "    super(Model_3DCNN, self).__init__()     \n",
        "    self.feat_dim = feat_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.use_cuda = use_cuda\n",
        "    \n",
        "    # SE block\n",
        "    self.conv_block1 = self.__conv_filter__(self.feat_dim, 64, 9, 2, 3)\n",
        "    self.glob_pool1 = nn.AdaptiveAvgPool3d(1)\n",
        "    self.SE_block1 = nn.Linear(in_features=64, out_features=64//16, bias=False)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.SE_block1_ = nn.Linear(in_features=64//16, out_features=64, bias=False)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # residual blocks\n",
        "    self.res_block1 = self.__conv_filter__(64, 64, 7, 1, 3)\n",
        "    self.res_block2 = self.__conv_filter__(64, 64, 7, 1, 3)\n",
        "\n",
        "    # SE block\n",
        "    self.conv_block2 = self.__conv_filter__(64, 128, 7, 3, 3)\n",
        "    self.glob_pool = nn.AdaptiveAvgPool3d(1)\n",
        "    self.SE_block2 = nn.Linear(in_features=128, out_features=128//16, bias=False)\n",
        "    self.SE_block2_ = nn.Linear(in_features=128//16, out_features=128, bias=False)\n",
        "    self.max_pool = nn.MaxPool3d(2)\n",
        "\n",
        "    ## SE block\n",
        "    self.conv_block3 = self.__conv_filter__(128, 256, 5, 2, 2)\n",
        "    self.SE_block3 = nn.Linear(in_features=256, out_features=256//16, bias=False)\n",
        "    self.SE_block3_ = nn.Linear(in_features=256//16, out_features=256, bias=False)\n",
        "\n",
        "    # dense layers\n",
        "    self.linear1 = nn.Linear(2048, 100)\n",
        "    torch.nn.init.normal_(self.linear1.weight, 0, 1)\n",
        "    self.linear1_bn = nn.BatchNorm1d(num_features=100, affine=True, momentum=0.1).train()\n",
        "    self.linear2 = nn.Linear(100, 1)\n",
        "    torch.nn.init.normal_(self.linear2.weight, 0, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    if x.dim() == 1:\n",
        "      x = x.unsqueeze(-1)\n",
        "\n",
        "    # SE block 1\n",
        "    conv1 = self.conv_block1(x)\n",
        "    a1,b1, _, _, _ = conv1.shape\n",
        "    glob_pool_conv1 = self.glob_pool(conv1).view(a1, b1)\n",
        "    SE_block1 = self.SE_block1(glob_pool_conv1)   \n",
        "    SE_block1a = self.relu(SE_block1)\n",
        "    SE_block1_ = self.SE_block1_(SE_block1a)\n",
        "    SE_block1_a = self.sigmoid(SE_block1_).view(a1, b1, 1, 1, 1)  \n",
        "    se1 = conv1 * SE_block1_a.expand_as(conv1)  \n",
        "\n",
        "    # residual blocks\n",
        "    conv1_res1 = self.res_block1(se1)\n",
        "    conv1_res12 = conv1_res1 + se1\n",
        "    conv1_res2 = self.res_block2(conv1_res12)\n",
        "    conv1_res2_2 = conv1_res2 + se1\n",
        "\n",
        "    # SE block 2\n",
        "    conv2 = self.conv_block2(conv1_res2_2)\n",
        "    a2,b2, _, _, _ = conv2.shape\n",
        "    glob_pool_conv2 = self.glob_pool(conv2).view(a2, b2)\n",
        "    SE_block2 = self.SE_block2(glob_pool_conv2)        \n",
        "    SE_block2a = self.relu(SE_block2)\n",
        "    SE_block2_ = self.SE_block2_(SE_block2a)\n",
        "    SE_block2_a = self.sigmoid(SE_block2_).view(a2, b2, 1, 1, 1)  \n",
        "    se2 = conv2 * SE_block2_a.expand_as(conv2)  \n",
        "\n",
        "    # Pooling layer\n",
        "    pool2 = self.max_pool(se2)\n",
        "\n",
        "    # SE block 3\n",
        "    conv3 = self.conv_block3(pool2)\n",
        "    a3,b3, _, _, _ = conv3.shape\n",
        "    glob_pool_conv3 = self.glob_pool(conv3).view(a3, b3)\n",
        "    SE_block3 = self.SE_block3(glob_pool_conv3)       \n",
        "    SE_block3a = self.relu(SE_block3)\n",
        "    SE_block3_ = self.SE_block3_(SE_block3a)\n",
        "    SE_block3_a = self.sigmoid(SE_block3_).view(a3, b3, 1, 1, 1)  \n",
        "    se3 = conv3 * SE_block3_a.expand_as(conv3)  \n",
        "\n",
        "    # Pooling layer\n",
        "    pool3 = se3\n",
        "\n",
        "    # Flatten\n",
        "    flatten = pool3.view(pool3.size(0), -1)\n",
        "\n",
        "    # Linear layer 1\n",
        "    linear1_z = self.linear1(flatten)\n",
        "    linear1_y = self.relu(linear1_z)\n",
        "    linear1 = self.linear1_bn(linear1_y) if linear1_y.shape[0]>1 else linear1_y\n",
        "\n",
        "    # Linear layer 2\n",
        "    linear2_z = self.linear2(linear1)\n",
        "\n",
        "    return linear2_z, flatten"
      ],
      "metadata": {
        "id": "FkrIYw7m1OH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Define fully-connected network class \"\"\"\n",
        "class Model_Linear(nn.Module):\n",
        "\tdef __init__(self, use_cuda=True):\n",
        "\t\tsuper(Model_Linear, self).__init__()     \n",
        "\t\tself.use_cuda = use_cuda\n",
        "\n",
        "\t\tself.fc1 = nn.Linear(2048, 100)\n",
        "\t\ttorch.nn.init.normal_(self.fc1.weight, 0, 1)\n",
        "\t\tself.dropout1 = nn.Dropout(0.0)\n",
        "\t\tself.fc1_bn = nn.BatchNorm1d(num_features=100, affine=True, momentum=0.3).train()\n",
        "\t\tself.fc2 = nn.Linear(100, 1)\n",
        "\t\ttorch.nn.init.normal_(self.fc2.weight, 0, 1)\n",
        "\t\tself.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tfc1_z = self.fc1(x)\n",
        "\t\tfc1_y = self.relu(fc1_z)\n",
        "\t\tfc1_d = self.dropout1(fc1_y)\n",
        "\t\tfc1 = self.fc1_bn(fc1_d) if fc1_d.shape[0]>1 else fc1_d  #batchnorm train require more than 1 batch\n",
        "\t\tfc2_z = self.fc2(fc1)\n",
        "\t\treturn fc2_z, fc1_z"
      ],
      "metadata": {
        "id": "UhpWZg7g05_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "mEQpCtKZ1gIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Define a function to train the GCN components\"\"\"\n",
        "\n",
        "def train_gcn(train_data, val_data, checkpoint_name, best_checkpoint_name, load_checkpoint_path = None, best_previous_checkpoint=None):\n",
        "\n",
        "    '''\n",
        "    Inputs:\n",
        "    1) train_data: training hdf file name\n",
        "    2) val_data: validation hdf file name\n",
        "    3) checkpoint_name: path to save checkpoint_name.pt\n",
        "    4) best_checkpoint_name: path to save best_checkpoint_name.pt\n",
        "    5) load_checkpoint_path: path to checkpoint file to load; default is None, i.e. training from scratch\n",
        "    6) best_previous_checkpoint: path to the best checkpoint from the previous round of training (required); default is None, i.e. training from scratch\n",
        "    Output:\n",
        "    1) checkpoint file, to load into testing function; saved as: checkpoint_dir + checkpoint_name\n",
        "    '''\n",
        "\n",
        "    # define train and validation hdf files\n",
        "    train_data_hdf = h5py.File(train_data, 'r')\n",
        "    val_data_hdf = h5py.File(val_data, 'r')\n",
        "\n",
        "    # define parameters\n",
        "    epochs = 300                   # number of training epochs\n",
        "    batch_size = 7                 # batch size to use for training\n",
        "    learning_rate = 0.001          # learning rate to use for training\n",
        "    gather_width = 128             # gather width\n",
        "    prop_iter = 4                  # number of propagation interations\n",
        "    dist_cutoff = 3.5              # common cutoff for donor-to-acceptor distance for energetically significant H bonds in proteins is 3.5 Ã…\n",
        "    feature_size = 20              # number of features: 19 + Van der Waals radius\n",
        "\n",
        "    # seed all random number generators and set cudnn settings for deterministic\n",
        "    random.seed(0)\n",
        "    np.random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    torch.cuda.manual_seed(0)\n",
        "    torch.cuda.manual_seed_all(0)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False \n",
        "    os.environ['PYTHONHASHSEED'] = '0'\n",
        "\n",
        "    def worker_init_fn(worker_id):\n",
        "        np.random.seed(int(0))\n",
        "        \n",
        "    # initialize checkpoint parameters\n",
        "    checkpoint_epoch = 0\n",
        "    checkpoint_step = 0\n",
        "    epoch_train_losses, epoch_val_losses, epoch_avg_corr = [], [], []\n",
        "    best_average_corr = float('-inf')\n",
        "\n",
        "    # define function to return checkpoint dictionary\n",
        "    def checkpoint_model(model, dataloader, epoch, step):\n",
        "        validate_dict = validate(model, dataloader)\n",
        "        model.train()\n",
        "        checkpoint_dict = {'model_state_dict': model.state_dict(), 'step': step, 'epoch': epoch, 'validate_dict': validate_dict,\n",
        "                           'epoch_train_losses': epoch_train_losses, 'epoch_val_losses': epoch_val_losses, 'epoch_avg_corr': epoch_avg_corr, 'best_avg_corr': best_average_corr}\n",
        "        torch.save(checkpoint_dict, checkpoint_name)\n",
        "        return checkpoint_dict\n",
        "\n",
        "    # define function to perform validation\n",
        "    def validate(model, val_dataloader):\n",
        "        # initialize\n",
        "        model.eval()\n",
        "        y_true = np.zeros((len(val_dataset),), dtype=np.float32)\n",
        "        y_pred = np.zeros((len(val_dataset),), dtype=np.float32)\n",
        "        # validation\n",
        "        for batch_ind, batch in enumerate(val_dataloader):\n",
        "            data_list = []\n",
        "            for dataset in batch:\n",
        "                pdbid = dataset[0]\n",
        "                affinity = val_data_hdf[pdbid].attrs['affinity'].reshape(1,-1)\n",
        "                vdw_radii = (val_data_hdf[pdbid].attrs['van_der_waals'].reshape(-1, 1))\n",
        "                node_feats = np.concatenate([vdw_radii, val_data_hdf[pdbid][:, 3:22]], axis=1)\n",
        "                edge_index, edge_attr = dense_to_sparse(torch.from_numpy(dataset[1]).float())\n",
        "                x = torch.from_numpy(node_feats).float()\n",
        "                y = torch.FloatTensor(affinity).view(-1, 1)\n",
        "                data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr.view(-1, 1), y=y)\n",
        "                data_list.append(data)\n",
        "            batch_data = [x for x in data_list]\n",
        "            y_ = model(batch_data)\n",
        "            y = torch.cat([x.y for x in data_list])\n",
        "            y_true[batch_ind*batch_size:batch_ind*batch_size+7] = y.cpu().float().data.numpy()[:,0]\n",
        "            y_pred[batch_ind*batch_size:batch_ind*batch_size+7] = y_.cpu().float().data.numpy()[:,0]\n",
        "            loss = criterion(y.float(), y_.cpu().float())\n",
        "            print('[%d/%d-%d/%d] validation loss: %.3f' % (epoch+1, epochs, batch_ind+1, len(val_dataset)//batch_size, loss))\n",
        "\n",
        "        # compute r^2\n",
        "        r2 = r2_score(y_true=y_true, y_pred=y_pred)\n",
        "        # compute mae\n",
        "        mae = mean_absolute_error(y_true=y_true, y_pred=y_pred)\n",
        "        # compute mse\n",
        "        mse = mean_squared_error(y_true=y_true, y_pred=y_pred)\n",
        "        # compute pearson correlation coefficient\n",
        "        pearsonr = stats.pearsonr(y_true.reshape(-1), y_pred.reshape(-1))[0]\n",
        "        # compte spearman correlation coefficient\n",
        "        spearmanr = stats.spearmanr(y_true.reshape(-1), y_pred.reshape(-1))[0]\n",
        "        # write out metrics\n",
        "        print('r2: {}\\tmae: {}\\trmse: {}\\tpearsonr: {}\\t spearmanr: {}'.format(r2, mae, mse**(1/2), pearsonr, spearmanr))\n",
        "        epoch_val_losses.append(mse)\n",
        "        epoch_avg_corr.append((pearsonr+spearmanr)/2)\n",
        "        model.train()\n",
        "        return {'r2': r2, 'mse': mse, 'mae': mae, 'pearsonr': pearsonr, 'spearmanr': spearmanr,\n",
        "                'y_true': y_true, 'y_pred': y_pred, 'best_average_corr': best_average_corr}\n",
        "   \n",
        "    # construct model\n",
        "    model = GeometricDataParallel(GCN(in_channels=feature_size, gather_width=gather_width, prop_iter=prop_iter, dist_cutoff=dist_cutoff)).float()\n",
        "\n",
        "    train_dataset = GCN_Dataset(data_file=train_data)\n",
        "    val_dataset = GCN_Dataset(data_file=val_data)\n",
        "        \n",
        "    # construct training and validation dataloaders to be fed to model\n",
        "    batch_count=len(train_dataset)\n",
        "    train_dataloader = DataListLoader(train_dataset, batch_size=batch_size, shuffle=True, worker_init_fn=worker_init_fn, drop_last=True)\n",
        "    val_dataloader = DataListLoader(val_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=worker_init_fn, drop_last=True)\n",
        "\n",
        "    # load checkpoint file\n",
        "    if load_checkpoint_path != None:\n",
        "        if torch.cuda.is_available():\n",
        "            model_train_dict = torch.load(load_checkpoint_path)\n",
        "            best_checkpoint = torch.load(best_previous_checkpoint)\n",
        "        else:\n",
        "            model_train_dict = torch.load(load_checkpoint_path, map_location=torch.device('cpu'))\n",
        "            best_checkpoint = torch.load(best_previous_checkpoint, map_location = torch.device('cpu'))\n",
        "        model.load_state_dict(model_train_dict['model_state_dict'])\n",
        "        checkpoint_epoch = model_train_dict['epoch']\n",
        "        checkpoint_step = model_train_dict['step']\n",
        "        epoch_train_losses = model_train_dict['epoch_train_losses']\n",
        "        epoch_val_losses = model_train_dict['epoch_val_losses']\n",
        "        epoch_avg_corr = model_train_dict['epoch_avg_corr']\n",
        "        val_dict = model_train_dict['validate_dict']\n",
        "        torch.save(best_checkpoint, best_checkpoint_name)\n",
        "        best_average_corr = best_checkpoint[\"best_avg_corr\"]\n",
        "        \n",
        "    model.train()\n",
        "    model.to(0)\n",
        "    \n",
        "    # set loss as MSE\n",
        "    criterion = nn.MSELoss().float()\n",
        "    # set Adam optimizer\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate) \n",
        "    \n",
        "    # train model\n",
        "    step = checkpoint_step\n",
        "    for epoch in range(checkpoint_epoch, epochs):\n",
        "        y_true = np.zeros((len(train_dataset),), dtype=np.float32)\n",
        "        y_pred = np.zeros((len(train_dataset),), dtype=np.float32)\n",
        "        for batch_ind, batch in enumerate(train_dataloader):\n",
        "            data_list = []\n",
        "            pdbid_array = []\n",
        "            for dataset in batch:\n",
        "                pdbid = dataset[0]\n",
        "                pdbid_array.append(pdbid)\n",
        "                affinity = train_data_hdf[pdbid].attrs['affinity'].reshape(1,-1)\n",
        "                vdw_radii = (train_data_hdf[pdbid].attrs['van_der_waals'].reshape(-1, 1))\n",
        "                node_feats = np.concatenate([vdw_radii, train_data_hdf[pdbid][:, 3:22]], axis=1)\n",
        "                edge_index, edge_attr = dense_to_sparse(torch.from_numpy(dataset[1]).float()) \n",
        "                x = torch.from_numpy(node_feats).float()\n",
        "                y = torch.FloatTensor(affinity).view(-1, 1)\n",
        "                data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr.view(-1, 1), y=y)\n",
        "                data_list.append(data)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            batch_data = [x for x in data_list]\n",
        "            y_ = model(batch_data)\n",
        "            y = torch.cat([x.y for x in data_list])\n",
        "            y_true[batch_ind*batch_size:batch_ind*batch_size+7] = y.cpu().float().data.numpy()[:,0]\n",
        "            y_pred[batch_ind*batch_size:batch_ind*batch_size+7] = y_.cpu().float().data.numpy()[:,0]\n",
        "\n",
        "            # compute loss and update parameters\n",
        "            loss = criterion(y.float(), y_.cpu().float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            step += 1\n",
        "            print(\"[%d/%d-%d/%d] training loss: %.3f\" % (epoch+1, epochs, batch_ind+1, len(train_dataset)//batch_size, loss))\n",
        "\n",
        "        r2 = r2_score(y_true=y_true, y_pred=y_pred)\n",
        "        mae = mean_absolute_error(y_true=y_true, y_pred=y_pred)\n",
        "        mse=mean_squared_error(y_true,y_pred)\n",
        "        epoch_train_losses.append(mse)\n",
        "        pearsonr = stats.pearsonr(y_true.reshape(-1), y_pred.reshape(-1))\n",
        "        spearmanr = stats.spearmanr(y_true.reshape(-1), y_pred.reshape(-1))\n",
        "\n",
        "        # write training summary for the epoch\n",
        "        print('epoch: {}\\trmse:{:0.4f}\\tr2: {:0.4f}\\t pearsonr: {:0.4f}\\tspearmanr: {:0.4f}\\tmae: {:0.4f}\\tpred'.format(epoch+1, mse**(1/2), r2, float(pearsonr[0]),\n",
        "                    float(spearmanr[0]), float(mae)))\n",
        "        \n",
        "        checkpoint_dict = checkpoint_model(model, val_dataloader, epoch+1, step)\n",
        "        if (checkpoint_dict[\"validate_dict\"][\"pearsonr\"] + checkpoint_dict[\"validate_dict\"][\"spearmanr\"])/2 > best_average_corr:\n",
        "          best_average_corr = (checkpoint_dict[\"validate_dict\"][\"pearsonr\"] + checkpoint_dict[\"validate_dict\"][\"spearmanr\"])/2\n",
        "          torch.save(checkpoint_dict, best_checkpoint_name)\n",
        "        torch.save(checkpoint_dict, checkpoint_name)\n",
        "          \n",
        "    # learning curve and correlation plot\n",
        "    fig, axs = plt.subplots(2)\n",
        "    axs[0].plot(np.arange(1, epochs+1), np.array(epoch_train_losses), label = 'training')\n",
        "    axs[0].plot(np.arange(1, epochs+1), np.array(epoch_val_losses), label = 'validation')\n",
        "    axs[0].set_xlabel('Epoch', fontsize=20)\n",
        "    axs[0].set_ylabel('Loss', fontsize=20)\n",
        "    axs[0].legend(fontsize=18)\n",
        "    axs[1].plot(np.arange(1, epochs+1), np.array(epoch_avg_corr))\n",
        "    axs[1].set_xlabel('Epoch', fontsize=20)\n",
        "    axs[1].set_ylabel('Validation Correlation', fontsize=20)\n",
        "    axs[1].set_ylim(0,1)\n",
        "    plt.show()\n",
        "  \n",
        "    train_data_hdf.close()\n",
        "    val_data_hdf.close()"
      ],
      "metadata": {
        "id": "TXH4tsua1icr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Define a function to train the 3D-CNN component\"\"\"\n",
        "\n",
        "def train_3dcnn(train_hdf, val_hdf, checkpoint_dir, best_checkpoint_dir, previous_checkpoint = None, best_previous_checkpoint=None):\n",
        "    '''\n",
        "    Inputs:\n",
        "    1) train_hdf: training hdf file name\n",
        "    2) val_hdf: validation hdf file name\n",
        "    3) checkpoint_dir: path to save checkpoint file: 'path/to/file.pt'\n",
        "    4) best_checkpoint_dir: path to save best checkpoint file: 'path/to/file.pt'\n",
        "    5) previous_checkpoint: path to the checkpoint at which training should be started; default = None, i.e. training from scratch\n",
        "    6) best_previous_checkpoint: path to the best checkpoint from the previous round of training (required); default = None, i.e. training from scratch\n",
        "    Output:\n",
        "    1) checkpoint file from the endpoint of the training\n",
        "    2) checkpoint file from the epoch with highest average correlation on validation\n",
        "    '''\n",
        "\n",
        "    # define parameters\n",
        "    batch_size = 50\n",
        "    learning_rate = .0007\n",
        "    learning_decay_iter=150\n",
        "    epoch_count = 100\n",
        "\n",
        "    # set CUDA for PyTorch\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device('cuda:0')\n",
        "    torch.cuda.set_device(0)\n",
        "\n",
        "    # initialize Datasets\n",
        "    dataset = CNN_Dataset(train_hdf)\n",
        "    val_dataset = CNN_Dataset(val_hdf)\n",
        "\n",
        "    # initialize Dataloaders\n",
        "    batch_count = len(dataset.data_info_list) // batch_size + 1\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # define model and helper functions\n",
        "    model = Model_3DCNN(use_cuda=use_cuda)\n",
        "    model.to(device)\n",
        "    loss_func = nn.MSELoss().float()\n",
        "    optimizer = RMSprop(model.parameters(), lr=learning_rate)\n",
        "    scheduler = lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.95)\n",
        "    \n",
        "    # initialize training variables\n",
        "    epoch_start = 0\n",
        "    step = 0\n",
        "    epoch_train_losses, epoch_val_losses, epoch_avg_corr = [], [], []\n",
        "    best_average_corr = float('-inf')\n",
        "\n",
        "    #load previous checkpoint if applicable\n",
        "    if previous_checkpoint!=None:\n",
        "        best_checkpoint = torch.load(best_previous_checkpoint, map_location = device)\n",
        "        torch.save(best_checkpoint, best_checkpoint_dir)\n",
        "        best_average_corr = best_checkpoint[\"best_avg_corr\"]\n",
        "        checkpoint = torch.load(previous_checkpoint, map_location=device)\n",
        "        model_state_dict = checkpoint.pop('model_state_dict')\n",
        "        model.load_state_dict(model_state_dict, strict=False)\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        epoch_start = checkpoint['epoch'] + 1\n",
        "        step=checkpoint['step']\n",
        "        epoch_train_losses = checkpoint['epoch_train_losses']\n",
        "        epoch_val_losses = checkpoint['epoch_val_losses']\n",
        "        epoch_avg_corr = checkpoint['epoch_avg_corr']\n",
        "        print('checkpoint loaded: %s' % previous_checkpoint)\n",
        "\n",
        "    def validate_model():\n",
        "        y_true_array = np.zeros((len(val_dataset),), dtype=np.float32)\n",
        "        y_pred_array = np.zeros((len(val_dataset),), dtype=np.float32)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch_ind, batch in enumerate(val_dataloader):\n",
        "               \n",
        "                # transfer to GPU\n",
        "                x_batch_cpu, y_batch_cpu, _ = batch\n",
        "                x_batch, y_batch = x_batch_cpu.to(device), y_batch_cpu.to(device)\n",
        "                ypred_batch, _ = model(x_batch[:x_batch.shape[0]])\n",
        "                \n",
        "                # compute and print batch loss\n",
        "                loss = loss_func(ypred_batch.cpu().float(), y_batch_cpu.float())\n",
        "                print('[%d/%d-%d/%d] validation loss: %.3f' % (epoch_ind+1, epoch_count, batch_ind+1, batch_count, loss.cpu().data.item()))\n",
        "                \n",
        "                #assemble the full datasets\n",
        "                bsize = x_batch.shape[0]\n",
        "                ytrue = y_batch_cpu.float().data.numpy()[:,0]\n",
        "                ypred = ypred_batch.cpu().float().data.numpy()[:,0]\n",
        "                y_true_array[batch_ind*batch_size:batch_ind*batch_size+bsize] = ytrue\n",
        "                y_pred_array[batch_ind*batch_size:batch_ind*batch_size+bsize] = ypred\n",
        "                \n",
        "            #compute average correlation\n",
        "            pearsonr = stats.pearsonr(y_true_array, y_pred_array)[0]\n",
        "            spearmanr = stats.spearmanr(y_true_array, y_pred_array)[0]\n",
        "            avg_corr = (pearsonr + spearmanr)/2\n",
        "        \n",
        "        # print information during training\n",
        "        print('[%d/%d] validation results-- pearsonr: %.3f, spearmanr: %.3f, rmse: %.3f, mae: %.3f, r2: %.3f' % (epoch_ind+1, epoch_count, pearsonr, spearmanr, mean_squared_error(y_true_array, \n",
        "                                                                                          y_pred_array)**(1/2), mean_absolute_error(y_true_array, y_pred_array), r2_score(y_true_array, y_pred_array)))\n",
        "        return mean_squared_error(y_true_array, y_pred_array), avg_corr\n",
        "\n",
        "    for epoch_ind in range(epoch_start, epoch_count):\n",
        "        x_batch = torch.zeros((batch_size,19,48,48,48)).float().to(device)\n",
        "        y_true_epoch = np.zeros((len(dataset),), dtype=np.float32)\n",
        "        y_pred_epoch = np.zeros((len(dataset),), dtype=np.float32)\n",
        "        for batch_ind, batch in enumerate(dataloader):\n",
        "            model.train()\n",
        "\n",
        "        # transfer to GPU and save in the epoch array\n",
        "            x_batch_cpu, y_batch_cpu, _ = batch\n",
        "            x_batch, y_batch = x_batch_cpu.to(device), y_batch_cpu.to(device)\n",
        "            bsize = x_batch.shape[0]\n",
        "            ypred_batch, _ = model(x_batch[:x_batch.shape[0]])\n",
        "            y_true_epoch[batch_ind*batch_size:batch_ind*batch_size+bsize] = y_batch_cpu.float().data.numpy()[:,0]\n",
        "            y_pred_epoch[batch_ind*batch_size:batch_ind*batch_size+bsize] = ypred_batch.cpu().float().data.numpy()[:,0]\n",
        "\n",
        "        # compute loss \n",
        "            loss = loss_func(ypred_batch.cpu().float(), y_batch_cpu.float())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            step += 1\n",
        "            print(\"[%d/%d-%d/%d] training loss: %.3f\" % (epoch_ind+1, epoch_count, batch_ind+1, batch_count, loss))\n",
        "\n",
        "        epoch_train_losses.append(mean_squared_error(y_true_epoch, y_pred_epoch))\n",
        "        val_loss, average_corr = validate_model()\n",
        "        epoch_val_losses.append(val_loss)\n",
        "        epoch_avg_corr.append(average_corr)\n",
        "        \n",
        "        checkpoint_dict = {'model_state_dict': model.state_dict(),'optimizer_state_dict': optimizer.state_dict(),'loss': loss,'step': step,'epoch': epoch_ind,\n",
        "                           'epoch_val_losses': epoch_val_losses,'epoch_train_losses': epoch_train_losses,'epoch_avg_corr' : epoch_avg_corr,'best_avg_corr': best_average_corr}\n",
        "\n",
        "        if (average_corr > best_average_corr):\n",
        "            best_average_corr = average_corr\n",
        "            checkpoint_dict[\"best_avg_corr\"] = best_average_corr\n",
        "            torch.save(checkpoint_dict, best_checkpoint_dir)\n",
        "            print(\"best checkpoint saved: %s\" % best_checkpoint_dir)\n",
        "        torch.save(checkpoint_dict, checkpoint_dir)\n",
        "        print('checkpoint saved: %s' % checkpoint_dir)\n",
        "    \n",
        "    # create learning curve and correlation plot\n",
        "    fig, axs = plt.subplots(2)\n",
        "    axs[0].plot(np.arange(1, epoch_count+1), np.array(epoch_train_losses), label = 'training')\n",
        "    axs[0].plot(np.arange(1, epoch_count+1), np.array(epoch_val_losses), label = 'validation')\n",
        "    axs[0].set_xlabel('Epoch', fontsize=20)\n",
        "    axs[0].set_ylabel('Loss', fontsize=20)\n",
        "    axs[0].legend(fontsize=18)\n",
        "    axs[1].plot(np.arange(1, epoch_count+1), np.array(epoch_avg_corr))\n",
        "    axs[1].set_xlabel('Epoch', fontsize=20)\n",
        "    axs[1].set_ylabel('Validation Correlation', fontsize=20)\n",
        "    axs[1].set_ylim(0,1)\n",
        "    plt.show()\n",
        "\n",
        "    # close dataset\n",
        "    dataset.close()\n",
        "    val_dataset.close()"
      ],
      "metadata": {
        "id": "Hs53kW-xNCCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Define a function to extract flattened features from trained 3D-CNN\"\"\"\n",
        "\n",
        "def savefeat_3dcnn(hdf_path, checkpoint_path, npy_path):\n",
        "\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    1) hdf_path: path/to/file.hdf\n",
        "    2) feature length: length of the flattened output features\n",
        "    3) checkpoint_path: path/to/checkpoint/file.pt\n",
        "    4) npy_path: path/to/save/features.npy\n",
        "    Output:\n",
        "    1) numpy file containing the saved features, with the last column being the true affinity value.\n",
        "    \"\"\"\n",
        "\n",
        "    # define parameters\n",
        "    multi_gpus = False\n",
        "    batch_size = 50\n",
        "    device_name = \"cuda:0\"\n",
        "    # set CUDA for PyTorch\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    cuda_count = torch.cuda.device_count()\n",
        "    if use_cuda:\n",
        "        device = torch.device(device_name)\n",
        "        torch.cuda.set_device(int(device_name.split(':')[1]))\n",
        "    else:   \n",
        "        device = torch.device(\"cpu\")\n",
        "    print(use_cuda, cuda_count, device)\n",
        "    # load testing \n",
        "    dataset = CNN_Dataset(hdf_path)\n",
        "    # check multi-gpus\n",
        "    num_workers = 0\n",
        "    if multi_gpus and cuda_count > 1:\n",
        "        num_workers = cuda_count\n",
        "    # initialize testing data loader\n",
        "    batch_count = len(dataset) // batch_size\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, worker_init_fn=None)\n",
        "    # define model\n",
        "    model = Model_3DCNN(use_cuda=use_cuda)\n",
        "    if multi_gpus and cuda_count > 1:\n",
        "        model = nn.DataParallel(model)\n",
        "    model.to(device)\n",
        "    if isinstance(model, (DistributedDataParallel, DataParallel)):\n",
        "        model = model.module\n",
        "    # load checkpoint file\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    # model state dict\n",
        "    model_state_dict = checkpoint.pop(\"model_state_dict\")\n",
        "    model.load_state_dict(model_state_dict, strict=False)\n",
        "    # create empty arrays to hold predicted and true values\n",
        "    ytrue_arr = np.zeros((len(dataset),), dtype=np.float32)\n",
        "    ypred_arr = np.zeros((len(dataset),), dtype=np.float32)\n",
        "    flatfeat_arr = np.zeros((len(dataset), 2048 + 1))\n",
        "    pdbid_arr = np.zeros((len(dataset),), dtype=object)\n",
        "    pred_list = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_ind, batch in enumerate(dataloader):\n",
        "            # transfer to GPU\n",
        "            x_batch_cpu, y_batch_cpu, pdbid_batch = batch\n",
        "            x_batch, y_batch = x_batch_cpu.to(device), y_batch_cpu.to(device)\n",
        "            # arrange and filter\n",
        "            bsize = x_batch.shape[0]\n",
        "            ypred_batch, flatfeat_batch = model(x_batch[:x_batch.shape[0]])\n",
        "            ytrue = y_batch_cpu.float().data.numpy()[:,0]\n",
        "            ypred = ypred_batch.cpu().float().data.numpy()[:,0]\n",
        "            flatfeat = flatfeat_batch.cpu().data.numpy()\n",
        "            ytrue_arr[batch_ind*batch_size:batch_ind*batch_size+bsize] = ytrue\n",
        "            ypred_arr[batch_ind*batch_size:batch_ind*batch_size+bsize] = ypred\n",
        "            flatfeat_arr[batch_ind*batch_size:batch_ind*batch_size+bsize, :-1] = flatfeat\n",
        "            pdbid_arr[batch_ind*batch_size:batch_ind*batch_size+bsize] = pdbid_batch\n",
        "    flatfeat_arr[:,-1] = ytrue_arr\n",
        "    np.save(npy_path, flatfeat_arr)"
      ],
      "metadata": {
        "id": "J81rsP5CNmc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Define a function to train the fully-connected network with extracted features from 3D-CNN\"\"\"\n",
        "\n",
        "def train_Linear(input_train_data, input_val_data, checkpoint_dir, best_checkpoint_dir, learning_decay_iter = 150, load_previous_checkpoint = False, previous_checkpoint = None, best_previous_checkpoint = None):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    1) input_train_data: path to train.npy data\n",
        "    2) input_val_data: path to val.npy data\n",
        "    3) checkpoint_dir: path to save checkpoint file: 'path/to/file.pt'\n",
        "    4) best_checkpoint_dir: path to save best checkpoint file: 'path/to/file.pt'\n",
        "    5) learning_decay_iter: frequency at which the learning rate is decreased by a multiplicative factor of decay_rate; set to 150 by default\n",
        "    6) load_previous_checkpoint: boolean variable indicating whether or not training should be started from an existing checkpoint. False by default\n",
        "    7) previous_checkpoint: path to the checkpoint at which training should be started. None by default.\n",
        "    8) best_previous_checkpoint: path to the checkpoint which was saved as \"best\" in the previous training. None by default\n",
        "    Outputs:\n",
        "    1) checkpoint file from the endpoint of the training\n",
        "    2) best checkpoint file, defined as that which maximizes the average of pearson and spearman correlations obtained from the validation data\n",
        "    \"\"\"\n",
        "\n",
        "    # define parameters\n",
        "    batch_size = 50\n",
        "    learning_rate = .0007\n",
        "    decay_iter = learning_decay_iter \n",
        "    decay_rate = 0.95\n",
        "    epoch_count = 100\n",
        "    checkpoint_iter = 343\n",
        "    device_name = \"cuda:0\"\n",
        "    multi_gpus = False\n",
        "\n",
        "    # set CUDA for PyTorch\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    cuda_count = torch.cuda.device_count()\n",
        "    if use_cuda:\n",
        "        device = torch.device(device_name)\n",
        "        torch.cuda.set_device(int(device_name.split(':')[1]))\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "    print(use_cuda, cuda_count, device)\n",
        "\n",
        "    def worker_init_fn(worker_id):\n",
        "        np.random.seed(int(0))\n",
        "\n",
        "    # build training dataset variable\n",
        "    dataset = Linear_Dataset(input_train_data)\n",
        "\n",
        "    # build validation dataset variable\n",
        "    val_dataset = Linear_Dataset(input_val_data)\n",
        "\n",
        "    # check multi-gpus\n",
        "    num_workers = 0\n",
        "    if multi_gpus and cuda_count > 1:\n",
        "        num_workers = cuda_count\n",
        "\n",
        "    def validate_model():\n",
        "        loss_fn = nn.MSELoss().float()\n",
        "        ytrue_arr = np.zeros((len(val_dataset),), dtype=np.float32)\n",
        "        ypred_arr = np.zeros((len(val_dataset),), dtype=np.float32)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch_ind, batch in enumerate(val_dataloader):\n",
        "        # transfer to GPU\n",
        "                x_batch_cpu, y_batch_cpu = batch\n",
        "                x_batch, y_batch = x_batch_cpu.to(device), y_batch_cpu.to(device)\n",
        "                ypred_batch, _ = model(x_batch[:x_batch.shape[0]])\n",
        "        # compute and print batch loss\n",
        "                loss = loss_fn(ypred_batch.cpu().float(), y_batch_cpu.float())\n",
        "        #assemble the full datasets\n",
        "                bsize = x_batch.shape[0]\n",
        "                ytrue = y_batch_cpu.float().data.numpy()[:,0]\n",
        "                ypred = ypred_batch.cpu().float().data.numpy()[:,0]\n",
        "                ytrue_arr[batch_ind*batch_size:batch_ind*batch_size+bsize] = ytrue\n",
        "                ypred_arr[batch_ind*batch_size:batch_ind*batch_size+bsize] = ypred            \n",
        "        #compute average correlation\n",
        "            pearsonr = stats.pearsonr(ytrue_arr, ypred_arr)[0]\n",
        "            spearmanr = stats.spearmanr(ytrue_arr, ypred_arr)[0]\n",
        "            avg_corr = (pearsonr + spearmanr)/2\n",
        "        return mean_squared_error(ytrue_arr, ypred_arr), avg_corr\n",
        "\n",
        "    # initialize training data loader\n",
        "    batch_count = len(dataset) // batch_size\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, worker_init_fn=None, shuffle=True)\n",
        "\n",
        "    # initialize validation data loader\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, worker_init_fn=None)\n",
        "\n",
        "    # define model\n",
        "    model = Model_Linear(use_cuda=use_cuda, verbose=verbose)\n",
        "    if multi_gpus and cuda_count > 1:\n",
        "        model = nn.DataParallel(model)\n",
        "    model.to(device)\n",
        "    if isinstance(model, (DistributedDataParallel, DataParallel)):\n",
        "        model = model.module\n",
        "\n",
        "    # define loss\n",
        "    loss_fn = nn.MSELoss().float()\n",
        "    # define optimizer\n",
        "    optimizer = RMSprop(model.parameters(), lr=learning_rate)\n",
        "    # define scheduler\n",
        "    scheduler = lr_scheduler.StepLR(optimizer, step_size=decay_iter, gamma=decay_rate)\n",
        "    # train model\n",
        "    epoch_start = 0\n",
        "    step = 0\n",
        "    epoch_train_losses, epoch_val_losses, epoch_avg_corr = [], [], []\n",
        "    best_average_corr = float('-inf')\n",
        "    best_checkpoint_dict = None\n",
        "    #load previous checkpoint if applicable\n",
        "    if load_previous_checkpoint:\n",
        "        best_checkpoint = torch.load(best_previous_checkpoint, map_location = device)\n",
        "        best_checkpoint_dict = best_checkpoint.pop(\"model_state_dict\")\n",
        "        best_average_corr = best_checkpoint[\"best_avg_corr\"]\n",
        "        checkpoint = torch.load(previous_checkpoint, map_location=device)\n",
        "        model_state_dict = checkpoint.pop(\"model_state_dict\")\n",
        "        model.load_state_dict(model_state_dict, strict=False)\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "        epoch_start = checkpoint[\"epoch\"]\n",
        "        loss = checkpoint[\"loss\"]\n",
        "        epoch_train_losses = checkpoint[\"epoch_train_losses\"]\n",
        "        epoch_val_losses = checkpoint[\"epoch_val_losses\"]\n",
        "        epoch_avg_corr = checkpoint[\"epoch_avg_corr\"]\n",
        "    for epoch_ind in range(epoch_start, epoch_count):\n",
        "        losses = []\n",
        "        for batch_ind, batch in enumerate(dataloader):\n",
        "            model.train()\n",
        "        # transfer to GPU\n",
        "            x_batch_cpu, y_batch_cpu = batch\n",
        "            x_batch, y_batch = x_batch_cpu.to(device), y_batch_cpu.to(device)\n",
        "            ypred_batch, _ = model(x_batch[:x_batch.shape[0]])\n",
        "        # compute loss\n",
        "            loss = loss_fn(ypred_batch.cpu().float(), y_batch_cpu.float())\n",
        "            losses.append(loss.cpu().data.item())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            val_loss, average_corr = validate_model()\n",
        "            checkpoint_dict = {\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"loss\": loss,\n",
        "                \"step\": step,\n",
        "                \"epoch\": epoch_ind,\n",
        "                \"epoch_val_losses\": epoch_val_losses,\n",
        "                \"epoch_train_losses\": epoch_train_losses,\n",
        "                \"epoch_avg_corr\" : epoch_avg_corr,\n",
        "                \"best_avg_corr\" : best_average_corr \n",
        "            }\n",
        "            if (average_corr > best_average_corr):\n",
        "                best_average_corr = average_corr\n",
        "                checkpoint_dict[\"best_avg_corr\"] = best_average_corr\n",
        "                best_checkpoint_dict = checkpoint_dict\n",
        "                torch.save(best_checkpoint_dict, best_checkpoint_dir)\n",
        "            torch.save(checkpoint_dict, checkpoint_dir)\n",
        "        step += 1\n",
        "    val_loss, average_corr = validate_model()\n",
        "    epoch_train_losses.append(np.mean(losses))\n",
        "    epoch_val_losses.append(val_loss)\n",
        "    epoch_avg_corr.append(average_corr)\n",
        "    checkpoint_dict = {\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"loss\": loss,\n",
        "                \"step\": step,\n",
        "                \"epoch\": epoch_ind,\n",
        "                \"epoch_val_losses\": epoch_val_losses,\n",
        "                \"epoch_train_losses\": epoch_train_losses,\n",
        "                \"epoch_avg_corr\" : epoch_avg_corr,\n",
        "                \"best_avg_corr\": best_average_corr\n",
        "            }\n",
        "    if (average_corr > best_average_corr):\n",
        "        best_average_corr = average_corr\n",
        "        checkpoint_dict[\"best_avg_corr\"] = best_average_corr\n",
        "        best_checkpoint_dict = checkpoint_dict\n",
        "        torch.save(best_checkpoint_dict, best_checkpoint_dir)\n",
        "    torch.save(checkpoint_dict, checkpoint_dir)"
      ],
      "metadata": {
        "id": "MxGMt4mUN5h2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "tyy60Y_d2DAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train GCN_0 for testing on PDBbind 2016 core set benchmark\n",
        "\n",
        "train_gcn(train_data = '/content/drive/MyDrive/HAC-Net/HAC-Net_files/train_val_test_files/MP-GCN/test_on_core_2016/2020_train_minus_core.hdf',\n",
        "          val_data = '/content/drive/MyDrive/HAC-Net/HAC-Net_files/train_val_test_files/MP-GCN/test_on_core_2016/2020_val_minus_core.hdf',\n",
        "          checkpoint_name = 'content/gcn0_core2016.pt',\n",
        "          best_checkpoint_name = 'content/best_gcn0_core2016.pt')"
      ],
      "metadata": {
        "id": "25MAKA_k1_sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train GCN_1 for testing on PDBbind 2016 core set benchmark\n",
        "\n",
        "train_gcn(train_data = '/content/drive/MyDrive/HAC-Net/HAC-Net_files/train_val_test_files/MP-GCN/test_on_core_2016/2020_train_minus_core.hdf',\n",
        "          val_data = '/content/drive/MyDrive/HAC-Net/HAC-Net_files/train_val_test_files/MP-GCN/test_on_core_2016/2020_val_minus_core.hdf',\n",
        "          checkpoint_name = 'content/gcn1_core2016.pt',\n",
        "          best_checkpoint_name = 'content/best_gcn1_core2016.pt')"
      ],
      "metadata": {
        "id": "HuAV3ELPPwph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train 3D-CNN for testing on PDBbind 2016 core set benchmark\n",
        "\n",
        "train_3dcnn(train_hdf = '/content/drive/MyDrive/HAC-Net/HAC-Net_files/train_val_test_files/3D-CNN/test_on_core_2016/vox_fixed_2020_train_minus_core.hdf',\n",
        "          val_hdf = '/content/drive/MyDrive/HAC-Net/HAC-Net_files/train_val_test_files/3D-CNN/test_on_core_2016/vox_fixed_2020_val_minus_core.hdf',\n",
        "          checkpoint_dir = 'content/3dcnn_core2016.pt',\n",
        "          best_checkpoint_dir = 'content/best_3dcnn_core2016.pt')"
      ],
      "metadata": {
        "id": "IESyh0KKP9HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Extract features from trained 3D-CNN\"\"\"\n",
        "\n",
        "# training set\n",
        "savefeat_3dcnn(hdf_path = '/content/drive/MyDrive/HAC-Net/HAC-Net_files/train_val_test_files/3D-CNN/test_on_core_2016/vox_fixed_2020_train_minus_core.hdf',\n",
        "               checkpoint_path = 'content/best_3dcnn_core2016.pt',\n",
        "               npy_path = 'content/3dcnn_features_train.npy')\n",
        "\n",
        "# validation set\n",
        "savefeat_3dcnn(hdf_path = '/content/drive/MyDrive/HAC-Net/HAC-Net_files/train_val_test_files/3D-CNN/test_on_core_2016/vox_fixed_2020_val_minus_core.hdf',\n",
        "               checkpoint_path = 'content/best_3dcnn_core2016.pt',\n",
        "               npy_path = 'content/3dcnn_features_val.npy')\n",
        "\n",
        "# test set\n",
        "savefeat_3dcnn(hdf_path = '/content/drive/MyDrive/HAC-Net/HAC-Net_files/train_val_test_files/3D-CNN/test_on_core_2016/vox_2016_core.hdf',\n",
        "               checkpoint_path = 'content/best_3dcnn_core2016.pt',\n",
        "               npy_path = 'content/3dcnn_features_test.npy')"
      ],
      "metadata": {
        "id": "rqe48VtTQdIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train fully-connected network for use in HAC-Net\n",
        "train_Linear(input_train_data = 'content/3dcnn_features_train.npy',\n",
        "             input_val_data = 'content/3dcnn_features_val.npy', \n",
        "             checkpoint_dir = 'content/3dcnn_fully-connected.pt', \n",
        "             best_checkpoint_dir = 'content/best_3dcnn_fully-connected.pt')"
      ],
      "metadata": {
        "id": "ZnKbGliIR3OP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}